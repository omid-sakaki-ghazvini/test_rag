# -*- coding: utf-8 -*-
"""test7 rag.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BrBzjj8CLYlVEygAekScfzRm92siC5UY
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install googletrans==3.1.0a0

# import libraries
import googletrans
from googletrans import Translator
import pandas as pd

# create a translator object
translator = Translator()

# use translate method to translate a string - by default, the destination language is english
translated = translator.translate('خرابی های مربوط به اتصال 19 ولت: لپتاب روشن نمیشود و روی منبع تغذیه 5 آمپر میکشد، خرابی در مدار محافظ یا بعد مدار محافظ هستش باید فیوز خروجی رو برداره ببینید اتصالی از کدوم سمت هست اگر از مدار محافظ باشه جک آداپتور یا قطعات مدار محافظ باشد  اگر مشکل از بعد از مدار محافظ باشد همه جاهایی که ۱۹ولت رفته باشه می‌تونه باعث اتصالی باشه باید تست دیودی خروجی مدار محافظ رو بگیریم و با تست دیودی تک تک سلف ها با کنفی مدار مقایسه کنیم اون سلفی که عدد یکسان نشون میده خرابی از اون مدار هست عدد صفر  بیشترین خرابی از خازن های مدار سی پی یو و گرافیک هست عدد نزدیک صفر ماسفت های مدار سی پی یو گرافیک یا رم می باشد دو طرف تست عدد یکسان از ماسفت های مدار شارژ یا مدار پاور می باشد')

# obtain translated string by using attribute .text

docs = [translated.text]

# Split the documents into smaller chunks
from langchain.text_splitter import CharacterTextSplitter

text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0, separator='', strip_whitespace=False)
documents = text_splitter.create_documents(docs)

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install -U langchain-community
# !pip install faiss-cpu

# Create embeddings for the document chunks using a pre-trained model
from langchain.embeddings import HuggingFaceEmbeddings

modelPath = "sentence-transformers/all-MiniLM-L6-v2"
model_kwargs = {'device': 'cpu'}
encode_kwargs = {'normalize_embeddings': False}

embeddings = HuggingFaceEmbeddings(
    model_name=modelPath,     # Provide the pre-trained model's path
    model_kwargs=model_kwargs, # Pass the model configuration options
    encode_kwargs=encode_kwargs # Pass the encoding options
)

# Create a FAISS vector store from the document embeddings
from langchain.vectorstores import FAISS

db = FAISS.from_documents(documents, embeddings)

# Function to load the GPT-2 model and create a text generation pipeline
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from langchain import HuggingFacePipeline

def getLlama():
    model_id = "openai-community/gpt2"
    print(f">> getLlama {model_id}")
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    model = AutoModelForCausalLM.from_pretrained(model_id)
    pipe = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        max_new_tokens=256,
        return_full_text=True,
        num_return_sequences=1,
        pad_token_id=tokenizer.eos_token_id,
    )
    llm = HuggingFacePipeline(pipeline=pipe)
    return llm

# Load the language model
llm = getLlama()

# Define a prompt template for generating responses
from langchain_core.prompts import PromptTemplate

prompt_sample = """
    I know context: {context}
    when asked: {question}
    my response using only information in the context is:
    """
prompt = PromptTemplate(template=prompt_sample, input_variables=["context", "question"])

# Create an LLM chain using the prompt template and language model
from langchain.chains import LLMChain

llm_chain = LLMChain(prompt=prompt, llm=llm)

import streamlit as st

# Ask a question and retrieve the context from the vector store

st.markdown('<p class="small-font">:لطفا سوال خود را در کادر زیر وارد نمایید</p>', unsafe_allow_html=True)
question = st.text_area('') 
#question = "لپتاب روشن نمیشه و روی منبع تغذیه ۵امپر می‌کشد چیکار باید کنم؟"
question = translator.translate(question)
question = question.text
doc_context = db.similarity_search(question)
#print(doc_context[0].page_content)

# Generate a response using the context and question
llm_response = llm_chain.invoke({"context": doc_context, "question": question})
answer = translator.translate(llm_response["text"], src='auto', dest='fa')
st.markdown('<p class="large-font">:پاسخ سوال شما براساس داده هایی که من به آن دسترسی دارم</p>', unsafe_allow_html=True)
#print(answer.text)
st.write(answer.text)

st.markdown("<br><hr><center>Made with ❤️ by <a href='https://omidsakaki.ir/'><strong>omid sakaki ghazvini</strong></a></center><hr>", unsafe_allow_html=True)
